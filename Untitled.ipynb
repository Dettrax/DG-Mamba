{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9792b40d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-06T17:45:32.915917Z",
     "start_time": "2024-06-06T17:45:32.857192Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nvcc: NVIDIA (R) Cuda compiler driver\n",
      "Copyright (c) 2005-2022 NVIDIA Corporation\n",
      "Built on Tue_May__3_18:49:52_PDT_2022\n",
      "Cuda compilation tools, release 11.7, V11.7.64\n",
      "Build cuda_11.7.r11.7/compiler.31294372_0\n"
     ]
    }
   ],
   "source": [
    "!nvcc --version\n",
    "import multiprocessing as mp\n",
    "mp.set_start_method('spawn', force=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f229614a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-06T17:45:35.644540Z",
     "start_time": "2024-06-06T17:45:32.915917Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "PyTorch version: 2.0.1\n",
      "CUDA version: 11.7\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print(torch.cuda.is_available())\n",
    "# Check PyTorch version\n",
    "torch_version = torch.__version__\n",
    "print(f\"PyTorch version: {torch_version}\")\n",
    "\n",
    "# Check CUDA version\n",
    "cuda_version = torch.version.cuda\n",
    "print(f\"CUDA version: {cuda_version}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "eaeafa81",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-06T17:45:35.660568Z",
     "start_time": "2024-06-06T17:45:35.644540Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of CPUs using os library: 128\n",
      "Number of logical CPUs using psutil library: 128\n",
      "Number of physical CPUs using psutil library: 128\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import psutil\n",
    "\n",
    "# Using os library\n",
    "num_cpus_os = os.cpu_count()\n",
    "print(f\"Number of CPUs using os library: {num_cpus_os}\")\n",
    "\n",
    "# Using psutil library\n",
    "num_logical_cpus_psutil = psutil.cpu_count()\n",
    "num_physical_cpus_psutil = psutil.cpu_count(logical=False)\n",
    "\n",
    "print(f\"Number of logical CPUs using psutil library: {num_logical_cpus_psutil}\")\n",
    "print(f\"Number of physical CPUs using psutil library: {num_physical_cpus_psutil}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "05f8f28e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-06T17:45:35.676245Z",
     "start_time": "2024-06-06T17:45:35.660568Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Memory: 503.48 GB\n",
      "Available Memory: 480.63 GB\n",
      "Used Memory: 19.67 GB\n",
      "Free Memory: 477.16 GB\n"
     ]
    }
   ],
   "source": [
    "import psutil\n",
    "\n",
    "# Get virtual memory details\n",
    "virtual_memory = psutil.virtual_memory()\n",
    "\n",
    "# Extract and print memory details\n",
    "total_memory = virtual_memory.total / (1024 ** 3)  # Convert bytes to GB\n",
    "available_memory = virtual_memory.available / (1024 ** 3)  # Convert bytes to GB\n",
    "used_memory = virtual_memory.used / (1024 ** 3)  # Convert bytes to GB\n",
    "free_memory = virtual_memory.free / (1024 ** 3)  # Convert bytes to GB\n",
    "\n",
    "print(f\"Total Memory: {total_memory:.2f} GB\")\n",
    "print(f\"Available Memory: {available_memory:.2f} GB\")\n",
    "print(f\"Used Memory: {used_memory:.2f} GB\")\n",
    "print(f\"Free Memory: {free_memory:.2f} GB\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7f63e40c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-06T17:45:37.112597Z",
     "start_time": "2024-06-06T17:45:35.676245Z"
    }
   },
   "outputs": [],
   "source": [
    "import torch_geometric.transforms as T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "66bb33ca",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-06T17:45:38.184837Z",
     "start_time": "2024-06-06T17:45:37.112597Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The current process ID is 1302763\n"
     ]
    }
   ],
   "source": [
    "import itertools\n",
    "import torch\n",
    "from torch.nn import ELU,Dropout\n",
    "import torch.nn.functional as F\n",
    "from torch.optim import Adam\n",
    "import torch.nn.utils as torch_utils\n",
    "import numpy as np\n",
    "import optuna\n",
    "from torch.nn.utils import clip_grad_norm_\n",
    "import math\n",
    "from mamba import Mamba, MambaConfig\n",
    "from tqdm import tqdm\n",
    "from sklearn.model_selection import KFold\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.metrics import precision_recall_curve\n",
    "from sklearn.metrics import auc, precision_recall_curve\n",
    "import time\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "# from mamba_ssm import Mamba # Importing Mamba model\n",
    "\n",
    "import torch\n",
    "from torch.nn import (\n",
    "    BatchNorm1d,\n",
    "    Embedding,\n",
    "    Linear,\n",
    "    ModuleList,\n",
    "    ReLU,\n",
    "    Sequential,\n",
    ")\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "import optuna\n",
    "from optuna.samplers import TPESampler\n",
    "from optuna.pruners import SuccessiveHalvingPruner\n",
    "import torch_geometric.transforms as T\n",
    "from torch_geometric.datasets import ZINC\n",
    "from torch_geometric.loader import DataLoader\n",
    "from torch_geometric.nn import GINEConv, global_add_pool\n",
    "import inspect\n",
    "from typing import Any, Dict, Optional\n",
    "\n",
    "import torch.nn.functional as F\n",
    "from torch import Tensor\n",
    "from torch.nn import Dropout, Linear, Sequential\n",
    "\n",
    "from torch_geometric.nn.conv import MessagePassing\n",
    "from torch_geometric.nn.inits import reset\n",
    "from torch_geometric.nn.resolver import (\n",
    "    activation_resolver,\n",
    "    normalization_resolver,\n",
    ")\n",
    "from torch_geometric.typing import Adj\n",
    "from torch_geometric.utils import to_dense_batch\n",
    "\n",
    "# from mamba_ssm import Mamba\n",
    "from torch_geometric.utils import degree, sort_edge_index\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import numpy as np\n",
    "import torch_geometric.transforms as T\n",
    "from torch_geometric.data import Data\n",
    "from torch_geometric.utils import dense_to_sparse\n",
    "\n",
    "\n",
    "np.random.seed(42)\n",
    "\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "import os\n",
    "\n",
    "# Get the current process ID\n",
    "pid = os.getpid()\n",
    "\n",
    "print(f\"The current process ID is {pid}\")\n",
    "\n",
    "# Load data from the uploaded CSV file into a Pandas DataFrame\n",
    "yeo_data = pd.read_csv('fmri_data.csv', delimiter='\\t', header=None)\n",
    "\n",
    "\n",
    "def get_dync(yeo_data,window_size,stride):\n",
    "    # Define Parameters:\n",
    "    num_rois = yeo_data.shape[0]\n",
    "    num_timepoints_per_session = 1200  # each session has 1200 timepoints\n",
    "    num_sessions = yeo_data.shape[1] // num_timepoints_per_session\n",
    "    num_windows_per_session = (num_timepoints_per_session - window_size) // stride + 1\n",
    "\n",
    "    # Compute dynFC for a Single Session\n",
    "    dynfc_matrices_list = []  # Initialize an empty list to store correlation matrices\n",
    "\n",
    "    # Choose the session for which you want to compute dynFC\n",
    "    chosen_session = 0  # first session index\n",
    "\n",
    "    # Iterate over time windows in the chosen session\n",
    "    for t in range(num_windows_per_session):\n",
    "        center_idx = chosen_session * num_timepoints_per_session + t + window_size // 2\n",
    "        start_idx = max(0, center_idx - window_size // 2)\n",
    "        end_idx = min(yeo_data.shape[1], center_idx + window_size // 2)\n",
    "\n",
    "        # Extract the window of data for the chosen session\n",
    "        window_data = yeo_data.iloc[:, start_idx:end_idx].values\n",
    "\n",
    "        # Compute Pearson correlation for the window_data\n",
    "        correlation_matrix = np.corrcoef(window_data, rowvar=True)\n",
    "\n",
    "        # Convert correlation matrix to binary matrix using threshold\n",
    "        threshold = 0.1\n",
    "        binary_matrix = (np.abs(correlation_matrix) > threshold).astype(int)\n",
    "\n",
    "        # Append the binary matrix to dynfc_matrices\n",
    "        dynfc_matrices_list.append(binary_matrix)\n",
    "\n",
    "    # Convert the list of matrices to a 3D numpy array\n",
    "    dynfc_matrices = np.array(dynfc_matrices_list)\n",
    "    data_len = dynfc_matrices.shape[0]\n",
    "\n",
    "    return dynfc_matrices,data_len\n",
    "\n",
    "class FMriDataset(Dataset):\n",
    "    def __init__(self, dynfc_matrices, lookback, walk_length=20):\n",
    "        self.dynfc_matrices = dynfc_matrices\n",
    "        self.lookback = lookback\n",
    "        self.data_len = dynfc_matrices.shape[0]\n",
    "        self.dataset, self.label = self.temp_process(dynfc_matrices, lookback)\n",
    "        self.transform = T.AddRandomWalkPE(walk_length=walk_length, attr_name='pe')\n",
    "\n",
    "    def temp_process(self, dynfc_matrices, lookback):\n",
    "        dataset = {}\n",
    "        label = {}\n",
    "        num_windows, num_nodes, _ = dynfc_matrices.shape\n",
    "\n",
    "        for i in range(lookback, num_windows):\n",
    "            B = np.zeros((num_nodes, lookback + 1, num_nodes))\n",
    "            for j in range(lookback + 1):\n",
    "                adj_matr = dynfc_matrices[i - lookback + j]\n",
    "                B[:, j, :] = adj_matr\n",
    "            dataset[i] = B\n",
    "            label[i] = dynfc_matrices[i]\n",
    "\n",
    "        return dataset, label\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataset)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        data = self.dataset[idx]\n",
    "        label = self.label[idx]\n",
    "        x = torch.tensor(data, dtype=torch.float32)\n",
    "\n",
    "        edge_index_list = []\n",
    "        edge_attr_list = []\n",
    "\n",
    "        for i in range(x.size(1)):  # Iterate over lookback + 1 adjacency matrices\n",
    "            adj_matrix = x[:, i, :]\n",
    "            edge_index, edge_attr = dense_to_sparse(adj_matrix)\n",
    "            edge_index_list.append(edge_index)\n",
    "            edge_attr_list.append(edge_attr)\n",
    "\n",
    "        edge_index = torch.cat(edge_index_list, dim=1)\n",
    "        edge_attr = torch.cat(edge_attr_list, dim=0)\n",
    "\n",
    "        graph_data = Data(x=torch.ones(x.size(0), 1), edge_index=edge_index, edge_attr=edge_attr)\n",
    "        graph_data = self.transform(graph_data)\n",
    "        batch = torch.zeros(x.size(0), dtype=torch.long)\n",
    "        pe = graph_data.pe  # Positional encodings\n",
    "\n",
    "        return x, pe, edge_index, edge_attr, batch,label\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "criterion = torch.nn.BCEWithLogitsLoss()\n",
    "# Define loss function\n",
    "def build_loss(mu,dynfc_matrices):\n",
    "    ground_truth = torch.tensor(dynfc_matrices, dtype=torch.float32).to(device)\n",
    "    loss = criterion(mu, ground_truth)\n",
    "    return loss\n",
    "\n",
    "\n",
    "def permute_within_batch(x, batch):\n",
    "    # Enumerate over unique batch indices\n",
    "    unique_batches = torch.unique(batch)\n",
    "\n",
    "    # Initialize list to store permuted indices\n",
    "    permuted_indices = []\n",
    "\n",
    "    for batch_index in unique_batches:\n",
    "        # Extract indices for the current batch\n",
    "        indices_in_batch = (batch == batch_index).nonzero().squeeze()\n",
    "\n",
    "        # Permute indices within the current batch\n",
    "        permuted_indices_in_batch = indices_in_batch[torch.randperm(len(indices_in_batch))]\n",
    "\n",
    "        # Append permuted indices to the list\n",
    "        permuted_indices.append(permuted_indices_in_batch)\n",
    "\n",
    "    # Concatenate permuted indices into a single tensor\n",
    "    permuted_indices = torch.cat(permuted_indices)\n",
    "\n",
    "    return permuted_indices\n",
    "\n",
    "\n",
    "class GPSConv(torch.nn.Module):\n",
    "\n",
    "    def __init__(\n",
    "            self,\n",
    "            channels: int,\n",
    "            conv: Optional[MessagePassing],\n",
    "            heads: int = 1,\n",
    "            dropout: float = 0.0,\n",
    "            attn_dropout: float = 0.0,\n",
    "            act: str = 'relu',\n",
    "            att_type: str = 'transformer',\n",
    "            order_by_degree: bool = False,\n",
    "            shuffle_ind: int = 0,\n",
    "            d_state: int = 16,\n",
    "            d_conv: int = 4,\n",
    "            act_kwargs: Optional[Dict[str, Any]] = None,\n",
    "            norm: Optional[str] = 'batch_norm',\n",
    "            norm_kwargs: Optional[Dict[str, Any]] = None,\n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "        self.channels = channels\n",
    "        self.conv = conv\n",
    "        self.heads = heads\n",
    "        self.dropout = dropout\n",
    "        self.att_type = att_type\n",
    "        self.shuffle_ind = shuffle_ind\n",
    "        self.order_by_degree = order_by_degree\n",
    "\n",
    "        assert (self.order_by_degree == True and self.shuffle_ind == 0) or (\n",
    "                    self.order_by_degree == False), f'order_by_degree={self.order_by_degree} and shuffle_ind={self.shuffle_ind}'\n",
    "\n",
    "        if self.att_type == 'transformer':\n",
    "            self.attn = torch.nn.MultiheadAttention(\n",
    "                channels,\n",
    "                heads,\n",
    "                dropout=attn_dropout,\n",
    "                batch_first=True,\n",
    "            )\n",
    "        config = MambaConfig(d_model=channels,d_state = d_state,d_conv=d_conv, n_layers=1,expand_factor=1)\n",
    "        if self.att_type == 'mamba':\n",
    "            self.self_attn = Mamba(config)\n",
    "\n",
    "        self.mlp = Sequential(\n",
    "            Linear(channels, channels),\n",
    "            activation_resolver(act, **(act_kwargs or {})),\n",
    "            Dropout(dropout),\n",
    "            Linear(channels, channels),\n",
    "            Dropout(dropout),\n",
    "        )\n",
    "\n",
    "        norm_kwargs = norm_kwargs or {}\n",
    "        self.norm1 = normalization_resolver(norm, channels, **norm_kwargs)\n",
    "        self.norm2 = normalization_resolver(norm, channels, **norm_kwargs)\n",
    "        self.norm3 = normalization_resolver(norm, channels, **norm_kwargs)\n",
    "\n",
    "        self.norm_with_batch = False\n",
    "        if self.norm1 is not None:\n",
    "            signature = inspect.signature(self.norm1.forward)\n",
    "            self.norm_with_batch = 'batch' in signature.parameters\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        r\"\"\"Resets all learnable parameters of the module.\"\"\"\n",
    "        if self.conv is not None:\n",
    "            self.conv.reset_parameters()\n",
    "        self.attn._reset_parameters()\n",
    "        reset(self.mlp)\n",
    "        if self.norm1 is not None:\n",
    "            self.norm1.reset_parameters()\n",
    "        if self.norm2 is not None:\n",
    "            self.norm2.reset_parameters()\n",
    "        if self.norm3 is not None:\n",
    "            self.norm3.reset_parameters()\n",
    "\n",
    "    def forward(\n",
    "            self,\n",
    "            x: Tensor,\n",
    "            edge_index: Adj,\n",
    "            batch: Optional[torch.Tensor] = None,\n",
    "            **kwargs,\n",
    "    ) -> Tensor:\n",
    "        r\"\"\"Runs the forward pass of the module.\"\"\"\n",
    "        hs = []\n",
    "        batch = batch.to(device)\n",
    "\n",
    "        if self.conv is not None:  # Local MPNN.\n",
    "            h = self.conv(x, edge_index, **kwargs)\n",
    "            h = F.dropout(h, p=self.dropout, training=self.training)\n",
    "            h = h + x\n",
    "            if self.norm1 is not None:\n",
    "                if self.norm_with_batch:\n",
    "                    h = self.norm1(h, batch=batch)\n",
    "                else:\n",
    "                    h = self.norm1(h)\n",
    "            hs.append(h)\n",
    "\n",
    "        ### Global attention transformer-style model.\n",
    "        if self.att_type == 'transformer':\n",
    "            h, mask = to_dense_batch(x, batch)\n",
    "            h, _ = self.attn(h, h, h, key_padding_mask=~mask, need_weights=False)\n",
    "            h = h[mask]\n",
    "\n",
    "        if self.att_type == 'mamba':\n",
    "\n",
    "            if self.order_by_degree:\n",
    "                deg = degree(edge_index[0], x.shape[0]).to(torch.long)\n",
    "                deg = deg.to(device)\n",
    "                order_tensor = torch.stack([batch, deg], 1).T\n",
    "                _, x = sort_edge_index(order_tensor, edge_attr=x)\n",
    "\n",
    "            if self.shuffle_ind == 0:\n",
    "                h, mask = to_dense_batch(x, batch)\n",
    "                h = self.self_attn(h)[mask]\n",
    "            else:\n",
    "                mamba_arr = []\n",
    "                for _ in range(self.shuffle_ind):\n",
    "                    h_ind_perm = permute_within_batch(x, batch)\n",
    "                    h_i, mask = to_dense_batch(x[h_ind_perm], batch)\n",
    "                    h_i = self.self_attn(h_i)[mask][h_ind_perm]\n",
    "                    mamba_arr.append(h_i)\n",
    "                h = sum(mamba_arr) / self.shuffle_ind\n",
    "        ###\n",
    "\n",
    "        h = F.dropout(h, p=self.dropout, training=self.training)\n",
    "        h = h + x  # Residual connection.\n",
    "        if self.norm2 is not None:\n",
    "            if self.norm_with_batch:\n",
    "                h = self.norm2(h, batch=batch)\n",
    "            else:\n",
    "                h = self.norm2(h)\n",
    "        hs.append(h)\n",
    "\n",
    "        out = sum(hs)  # Combine local and global outputs.\n",
    "\n",
    "        out = out + self.mlp(out)\n",
    "        if self.norm3 is not None:\n",
    "            if self.norm_with_batch:\n",
    "                out = self.norm3(out, batch=batch)\n",
    "            else:\n",
    "                out = self.norm3(out)\n",
    "\n",
    "        return out\n",
    "\n",
    "    def __repr__(self) -> str:\n",
    "        return (f'{self.__class__.__name__}({self.channels}, '\n",
    "                f'conv={self.conv}, heads={self.heads})')\n",
    "\n",
    "\n",
    "class GraphModel(torch.nn.Module):\n",
    "    def __init__(self, channels: int, pe_dim: int, num_layers: int, model_type: str, shuffle_ind: int, d_state: int,\n",
    "                 d_conv: int,dropout:float, order_by_degree: False,window_size_emd:int,walk_length:int,lookback:int):\n",
    "        super().__init__()\n",
    "        self.channels = channels\n",
    "        self.node_emb = Embedding(window_size_emd\n",
    "                                  , channels)\n",
    "        self.pe_lin = Linear(walk_length, pe_dim)\n",
    "        self.pe_norm = BatchNorm1d(walk_length)\n",
    "        self.edge_emb = Embedding(4, channels)\n",
    "        self.model_type = model_type\n",
    "        self.shuffle_ind = shuffle_ind\n",
    "        self.order_by_degree = order_by_degree\n",
    "        self.dropout = dropout\n",
    "        self.dropout_ly = Dropout(dropout)\n",
    "        self.convs = ModuleList()\n",
    "        for _ in range(num_layers):\n",
    "            nn = Sequential(\n",
    "                Linear(channels, int(channels)),\n",
    "                ReLU(),\n",
    "                Linear(int(channels), self.channels),\n",
    "            )\n",
    "            if self.model_type == 'gine':\n",
    "                conv = GINEConv(nn)\n",
    "\n",
    "            if self.model_type == 'mamba':\n",
    "                conv = GPSConv(channels, GINEConv(nn), heads=4, attn_dropout=self.dropout,\n",
    "                               att_type='mamba',\n",
    "                               shuffle_ind=self.shuffle_ind,\n",
    "                               order_by_degree=self.order_by_degree,\n",
    "                               d_state=d_state, d_conv=d_conv)\n",
    "\n",
    "            if self.model_type == 'transformer':\n",
    "                conv = GPSConv(channels, GINEConv(nn), heads=4, attn_dropout=0.5, att_type='transformer')\n",
    "\n",
    "\n",
    "            # conv = GINEConv(nn)\n",
    "            self.convs.append(conv)\n",
    "\n",
    "        self.mu = Linear(channels, 100)\n",
    "        # self.sigma = Linear(channels, 100)\n",
    "        self.elu = ELU()\n",
    "        self.convs_norm = ModuleList([BatchNorm1d(channels) for _ in range(num_layers)])\n",
    "        self.node_emb_norm = BatchNorm1d((lookback+1) * window_size).to(device)\n",
    "        self.pe_lin_norm = BatchNorm1d(pe_dim)\n",
    "        self.x_linear = Linear(pe_dim +(channels * (lookback+1) * window_size), self.channels).to(device)\n",
    "        self.node_emb_norm_after = BatchNorm1d(channels).to(device)\n",
    "\n",
    "    def forward(self, x, pe, edge_index, edge_attr, batch):\n",
    "\n",
    "        x_pe_norm = self.pe_norm(pe)\n",
    "\n",
    "        x_flat = x.view(x.size(0), -1)  # Flatten x to 2D\n",
    "\n",
    "\n",
    "\n",
    "        x_after_norm = self.node_emb_norm(x_flat)\n",
    "\n",
    "        x_min, x_max = x_after_norm.min(), x_after_norm.max()\n",
    "        x_after_min_max = (x_after_norm - x_min) / (x_max - x_min)  # Normalize to [0, 1]\n",
    "        x_conv = (x_after_min_max * (self.node_emb.num_embeddings - 1)).int()  # Scale to [0, vocab_size-1] and convert to long\n",
    "\n",
    "\n",
    "\n",
    "        x_emb = self.node_emb(x_conv)\n",
    "        x_emb_view = x_emb.view(x_emb.size(0), -1)\n",
    "\n",
    "\n",
    "        x_pe_lin = self.pe_lin(x_pe_norm)\n",
    "        x_pe_norm = self.pe_lin_norm(x_pe_lin)\n",
    "        x_pe_drop = self.dropout_ly(x_pe_norm)\n",
    "\n",
    "\n",
    "        x_cat = torch.cat((x_emb_view, x_pe_drop), dim=1)\n",
    "        # x_linear = Linear(x_cat.shape[1], self.channels).to(device)\n",
    "        x_after_linear = self.x_linear(x_cat)\n",
    "\n",
    "        \n",
    "        x_after_norm = self.node_emb_norm_after(x_after_linear)\n",
    "        x_after_drop = self.dropout_ly(x_after_norm)\n",
    "        edge_attr = self.edge_emb(edge_attr.int())\n",
    "\n",
    "        torch.cuda.empty_cache()\n",
    "        for i, conv in enumerate(self.convs):\n",
    "            if self.model_type == 'gine':\n",
    "                x_after_drop = conv(x_after_drop, edge_index, edge_attr=edge_attr)\n",
    "            else:\n",
    "                x_after_drop = conv(x_after_drop, edge_index, batch, edge_attr=edge_attr)\n",
    "            x_after_norm = self.convs_norm[i](x_after_drop)\n",
    "            x_after_drop = self.dropout_ly(x_after_norm)\n",
    "\n",
    "\n",
    "        mu = torch.sigmoid(self.mu(x_after_drop))\n",
    "        # sigma = self.sigma(x_after_drop)\n",
    "        # sigma = self.elu(sigma) + 1 + 1e-14\n",
    "        return mu, None\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def optimise_mamba(lookback,window_size,stride,channel,pe_dim,num_layers,d_conv,d_state,dropout,lr,weight_decay,walk_length):\n",
    "\n",
    "\n",
    "        dynfc_matrices,data_len = get_dync(yeo_data,window_size,stride)\n",
    "        \n",
    "        # Create dataset\n",
    "        dataset = FMriDataset(dynfc_matrices, lookback,walk_length)\n",
    "        \n",
    "        \n",
    "        \n",
    "        model = GraphModel(channels=channel, pe_dim=pe_dim, num_layers=num_layers,\n",
    "                           model_type='mamba',\n",
    "                           shuffle_ind=0, order_by_degree=True,\n",
    "                           d_conv=d_conv, d_state=d_state,dropout=dropout,window_size_emd=window_size,walk_length=walk_length,lookback=lookback\n",
    "                           ).to(device)\n",
    "        \n",
    "        \n",
    "        optimizer = torch.optim.Adam(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "        \n",
    "        \n",
    "        \n",
    "        x_dict = {}\n",
    "        pe_dict = {}\n",
    "        edge_index_dict = {}\n",
    "        edge_attr_dict = {}\n",
    "        batch_dict = {}\n",
    "        label_dict = {}\n",
    "        for i in range(lookback, int((0.7 * data_len)*1)):\n",
    "            x, pe, edge_index, edge_attr, batch, label = dataset[i]\n",
    "            x = x.clone().detach().requires_grad_(True).to(device)\n",
    "            pe = pe.clone().detach().requires_grad_(True).to(device)\n",
    "            edge_index = edge_index.clone().detach().to(device)\n",
    "            edge_attr = edge_attr.clone().detach().requires_grad_(True).to(device)\n",
    "            batch = batch.clone().detach().to(device)\n",
    "            x_dict[i] = x\n",
    "            pe_dict[i] = pe\n",
    "            edge_index_dict[i] = edge_index\n",
    "            edge_attr_dict[i] = edge_attr\n",
    "            batch_dict[i] = batch\n",
    "            label = x[:, -1, :]\n",
    "            label_dict[i] = label\n",
    "        \n",
    "        model.train()\n",
    "        for e in tqdm(range(10)):\n",
    "            loss_step = []\n",
    "            for i in range(lookback, int((0.7 * data_len)*1)):\n",
    "                    x, pe, edge_index, edge_attr, batch,label = x_dict[i], pe_dict[i], edge_index_dict[i], edge_attr_dict[i], batch_dict[i], label_dict[i]\n",
    "                    optimizer.zero_grad()\n",
    "                    mu, sigma = model(x, pe, edge_index, edge_attr,\n",
    "                        batch)\n",
    "        \n",
    "                    loss = build_loss(mu, label)\n",
    "        \n",
    "                    loss_step.append(loss.cpu().detach().numpy())\n",
    "                    loss.backward()\n",
    "                    clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "                    optimizer.step()\n",
    "                    optimizer.zero_grad()  # Zero the gradients after the optimization step\n",
    "                    del x, pe, edge_index, edge_attr, batch, mu, sigma,loss  # Delete variables that are no longer in use\n",
    "                    torch.cuda.empty_cache()  # Free up memory\n",
    "            # print(f\"Epoch {e} Loss: {np.mean(np.stack(loss_step))}\")\n",
    "        model.eval()\n",
    "        val_loss_value = 0.0\n",
    "        val_samples = 0\n",
    "        for i in range(int(0.7 * data_len), int(0.85 * data_len)):\n",
    "                x, pe, edge_index, edge_attr, batch, label = dataset[i]\n",
    "                label = x[:,-1,:]\n",
    "                x = x.clone().detach().requires_grad_(True).to(device)\n",
    "                pe = pe.clone().detach().requires_grad_(True).to(device)\n",
    "                edge_index = edge_index.clone().detach().to(device)\n",
    "                edge_attr = edge_attr.clone().detach().requires_grad_(True).to(device)\n",
    "                mu, sigma = model(x, pe, edge_index, edge_attr,\n",
    "                    batch)\n",
    "        \n",
    "                val_loss_value += build_loss(mu,label).item()\n",
    "                val_samples += 1\n",
    "        val_loss_value /= val_samples\n",
    "        # print(f\"Validation Loss: {val_loss_value}\")\n",
    "        \n",
    "        predictions_list = []\n",
    "        ground_truth_list = []\n",
    "        \n",
    "        # Set model to evaluation mode\n",
    "        model.eval()\n",
    "        \n",
    "        # Evaluate MAP for lookback = 1\n",
    "        AP_list = []\n",
    "        for i in range(int(0.85 * data_len), data_len):\n",
    "            # Get input data\n",
    "            x, pe, edge_index, edge_attr, batch, label = dataset[i]\n",
    "            label = x[:,-1,:]\n",
    "            x = x.clone().detach().requires_grad_(True).to(device)\n",
    "            pe = pe.clone().detach().requires_grad_(True).to(device)\n",
    "            edge_index = edge_index.clone().detach().to(device)\n",
    "            edge_attr = edge_attr.clone().detach().requires_grad_(True).to(device)\n",
    "            # Forward pass\n",
    "            with torch.no_grad():\n",
    "                mu_pred, sigma_pred = model(x, pe, edge_index, edge_attr,\n",
    "                    batch)\n",
    "        \n",
    "        \n",
    "            # Assuming you want to use the means as predictions\n",
    "            predictions = mu_pred.cpu().numpy()\n",
    "            ground_truth = label.cpu().numpy().astype(int)  # Use binary FC matrices as ground truth\n",
    "        \n",
    "            # Reshape predictions and ground truth to match the shape of predictions_flat\n",
    "            predictions_flat = predictions.reshape(predictions.shape[0], -1)\n",
    "            ground_truth_flat = ground_truth.reshape(ground_truth.shape[0], -1)\n",
    "        \n",
    "            # Reshape ground_truth_flat to match the shape of predictions_flat\n",
    "            ground_truth_flat_resized = ground_truth_flat[:, :predictions_flat.shape[1]]\n",
    "        \n",
    "            # Compute AP for each sample\n",
    "            AP_sample_list = []\n",
    "            for j in range(predictions_flat.shape[0]):\n",
    "                precision, recall, _ = precision_recall_curve(ground_truth_flat_resized[j], predictions_flat[j])\n",
    "                AP = auc(recall, precision)\n",
    "                AP_sample_list.append(AP)\n",
    "        \n",
    "            # Compute AP for the current sample and append to list\n",
    "            AP_list.append(np.mean(AP_sample_list))\n",
    "        \n",
    "        # Compute MAP for lookback = 5\n",
    "        MAP_value = np.mean(AP_list)\n",
    "        return MAP_value\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d54ff58e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-06T17:45:38.200969Z",
     "start_time": "2024-06-06T17:45:38.185343Z"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bab01494",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-06T17:45:38.312627Z",
     "start_time": "2024-06-06T17:45:38.200969Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-06-08 01:16:27,354] A new study created in memory with name: no-name-99f1da71-1a35-425a-8da0-85242f8eb4a4\n",
      "[I 2024-06-08 01:22:02,505] Trial 2 finished with value: 0.843614823172939 and parameters: {'lookback': 1, 'stride': 2, 'channel': 32, 'pe_dim': 6, 'num_layer': 2, 'd_conv': 6, 'd_state': 4, 'dropout': 0.4119769385969464, 'lr': 0.0015656101792536044, 'weight_decay': 0.036387545359478544, 'walk': 14}. Best is trial 2 with value: 0.843614823172939.\n",
      "[I 2024-06-08 01:22:41,878] Trial 4 finished with value: 0.8496056727936854 and parameters: {'lookback': 3, 'stride': 2, 'channel': 16, 'pe_dim': 4, 'num_layer': 2, 'd_conv': 6, 'd_state': 16, 'dropout': 0.4641490581134818, 'lr': 0.003326388699431873, 'weight_decay': 0.031151507897586972, 'walk': 16}. Best is trial 4 with value: 0.8496056727936854.\n",
      "[I 2024-06-08 01:24:19,746] Trial 9 finished with value: 0.8762735606658292 and parameters: {'lookback': 3, 'stride': 2, 'channel': 32, 'pe_dim': 2, 'num_layer': 2, 'd_conv': 4, 'd_state': 16, 'dropout': 0.21766649951741507, 'lr': 0.008868057013229642, 'weight_decay': 0.09381390337147469, 'walk': 20}. Best is trial 9 with value: 0.8762735606658292.\n",
      "[I 2024-06-08 01:25:26,351] Trial 8 finished with value: 0.8570384146884205 and parameters: {'lookback': 5, 'stride': 2, 'channel': 16, 'pe_dim': 4, 'num_layer': 2, 'd_conv': 4, 'd_state': 16, 'dropout': 0.3566021871567021, 'lr': 0.008974947940584212, 'weight_decay': 0.03723025447312848, 'walk': 16}. Best is trial 9 with value: 0.8762735606658292.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics import precision_recall_curve\n",
    "from sklearn.metrics import auc, precision_recall_curve\n",
    "import time\n",
    "from scipy.stats import uniform\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "# Define the objective function\n",
    "def objective(trial):\n",
    "    # Define the hyperparameters to optimize\n",
    "    lookback = trial.suggest_categorical('lookback', [1,3,5])\n",
    "    window_size = 100\n",
    "    stride = trial.suggest_categorical('stride', [1,2])\n",
    "    channel = trial.suggest_categorical('channel', [16,32,64])\n",
    "    pe_dim = trial.suggest_categorical('pe_dim', [2,4,6,8])\n",
    "    num_layers = trial.suggest_categorical('num_layer', [1,2])\n",
    "    d_conv = trial.suggest_categorical('d_conv', [2,4,6])\n",
    "    d_state = trial.suggest_categorical('d_state', [2,4,8,16])\n",
    "    dropout = trial.suggest_float('dropout', 0.2, 0.5)\n",
    "    lr = trial.suggest_float('lr', 1e-3, 1e-2)\n",
    "    weight_decay = trial.suggest_float('weight_decay', 1e-3, 1e-1)\n",
    "    walk_length = trial.suggest_int('walk',1,20)\n",
    "\n",
    "    # Optimize the model using the current hyperparameters\n",
    "    val_loss = optimise_mamba(lookback,window_size,stride,channel,pe_dim,num_layers,d_conv,d_state,dropout,lr,weight_decay,walk_length)\n",
    "\n",
    "    return val_loss\n",
    "\n",
    "start_time = time.time()\n",
    "sampler = TPESampler(seed=10)\n",
    "pruner = SuccessiveHalvingPruner()\n",
    "study = optuna.create_study(direction='maximize', sampler=sampler,pruner=pruner)\n",
    "study.optimize(objective, n_trials=10,n_jobs=-1)\n",
    "end_time = time.time()\n",
    "print(f\"Execution time: {end_time - start_time} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49494d00",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-06T17:45:38.328679Z",
     "start_time": "2024-06-06T17:45:38.313635Z"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10fbb654",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-06T17:45:43.947879Z",
     "start_time": "2024-06-06T17:45:38.328679Z"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ef7a8f2c",
   "metadata": {
    "jupyter": {
     "is_executing": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hola\n"
     ]
    }
   ],
   "source": [
    "print(\"Hola\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "413dac2c1c84fe3d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# from dask.distributed import LocalCluster , Client\n",
    "# cluster = LocalCluster()\n",
    "# client = Client(cluster)\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ce8e4e28",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from optuna.integration.dask import DaskStorage\n",
    "# study = optuna.create_study(storage=DaskStorage())\n",
    "\n",
    "# futures = [client.submit(study.optimize , objective , n_trials = 1 , pure =False)\n",
    "#            for _ in range(10)\n",
    "#           ]\n",
    "# client.gather(futures)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "31116e1a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-06-06 17:32:15,394] A new study created in memory with name: no-name-1595683e-1aca-47e8-a000-5c084c682bc4\n",
      "Traceback (most recent call last):\n",
      "  File \"<string>\", line 1, in <module>\n",
      "  File \"/home/ap2934/.conda/envs/torch-cuda/lib/python3.8/multiprocessing/spawn.py\", line 116, in spawn_main\n",
      "    exitcode = _main(fd, parent_sentinel)\n",
      "  File \"/home/ap2934/.conda/envs/torch-cuda/lib/python3.8/multiprocessing/spawn.py\", line 126, in _main\n",
      "    self = reduction.pickle.load(from_parent)\n",
      "AttributeError: Can't get attribute 'objective' on <module '__main__' (built-in)>\n",
      "Traceback (most recent call last):\n",
      "  File \"<string>\", line 1, in <module>\n",
      "  File \"/home/ap2934/.conda/envs/torch-cuda/lib/python3.8/multiprocessing/spawn.py\", line 116, in spawn_main\n",
      "    exitcode = _main(fd, parent_sentinel)\n",
      "  File \"/home/ap2934/.conda/envs/torch-cuda/lib/python3.8/multiprocessing/spawn.py\", line 126, in _main\n",
      "    self = reduction.pickle.load(from_parent)\n",
      "AttributeError: Can't get attribute 'objective' on <module '__main__' (built-in)>\n",
      "Traceback (most recent call last):\n",
      "  File \"<string>\", line 1, in <module>\n",
      "  File \"/home/ap2934/.conda/envs/torch-cuda/lib/python3.8/multiprocessing/spawn.py\", line 116, in spawn_main\n",
      "    exitcode = _main(fd, parent_sentinel)\n",
      "  File \"/home/ap2934/.conda/envs/torch-cuda/lib/python3.8/multiprocessing/spawn.py\", line 126, in _main\n",
      "    self = reduction.pickle.load(from_parent)\n",
      "AttributeError: Can't get attribute 'objective' on <module '__main__' (built-in)>\n",
      "Traceback (most recent call last):\n",
      "  File \"<string>\", line 1, in <module>\n",
      "  File \"/home/ap2934/.conda/envs/torch-cuda/lib/python3.8/multiprocessing/spawn.py\", line 116, in spawn_main\n",
      "    exitcode = _main(fd, parent_sentinel)\n",
      "  File \"/home/ap2934/.conda/envs/torch-cuda/lib/python3.8/multiprocessing/spawn.py\", line 126, in _main\n",
      "    self = reduction.pickle.load(from_parent)\n",
      "AttributeError: Can't get attribute 'objective' on <module '__main__' (built-in)>\n",
      "Traceback (most recent call last):\n",
      "  File \"<string>\", line 1, in <module>\n",
      "  File \"/home/ap2934/.conda/envs/torch-cuda/lib/python3.8/multiprocessing/spawn.py\", line 116, in spawn_main\n",
      "    exitcode = _main(fd, parent_sentinel)\n",
      "  File \"/home/ap2934/.conda/envs/torch-cuda/lib/python3.8/multiprocessing/spawn.py\", line 126, in _main\n",
      "    self = reduction.pickle.load(from_parent)\n",
      "AttributeError: Can't get attribute 'objective' on <module '__main__' (built-in)>\n",
      "Traceback (most recent call last):\n",
      "  File \"<string>\", line 1, in <module>\n",
      "  File \"/home/ap2934/.conda/envs/torch-cuda/lib/python3.8/multiprocessing/spawn.py\", line 116, in spawn_main\n",
      "    exitcode = _main(fd, parent_sentinel)\n",
      "  File \"/home/ap2934/.conda/envs/torch-cuda/lib/python3.8/multiprocessing/spawn.py\", line 126, in _main\n",
      "    self = reduction.pickle.load(from_parent)\n",
      "AttributeError: Can't get attribute 'objective' on <module '__main__' (built-in)>\n",
      "Traceback (most recent call last):\n",
      "  File \"<string>\", line 1, in <module>\n",
      "  File \"/home/ap2934/.conda/envs/torch-cuda/lib/python3.8/multiprocessing/spawn.py\", line 116, in spawn_main\n",
      "    exitcode = _main(fd, parent_sentinel)\n",
      "  File \"/home/ap2934/.conda/envs/torch-cuda/lib/python3.8/multiprocessing/spawn.py\", line 126, in _main\n",
      "    self = reduction.pickle.load(from_parent)\n",
      "AttributeError: Can't get attribute 'objective' on <module '__main__' (built-in)>\n",
      "Traceback (most recent call last):\n",
      "  File \"<string>\", line 1, in <module>\n",
      "  File \"/home/ap2934/.conda/envs/torch-cuda/lib/python3.8/multiprocessing/spawn.py\", line 116, in spawn_main\n",
      "    exitcode = _main(fd, parent_sentinel)\n",
      "  File \"/home/ap2934/.conda/envs/torch-cuda/lib/python3.8/multiprocessing/spawn.py\", line 126, in _main\n",
      "    self = reduction.pickle.load(from_parent)\n",
      "AttributeError: Can't get attribute 'objective' on <module '__main__' (built-in)>\n",
      "Traceback (most recent call last):\n",
      "  File \"<string>\", line 1, in <module>\n",
      "  File \"/home/ap2934/.conda/envs/torch-cuda/lib/python3.8/multiprocessing/spawn.py\", line 116, in spawn_main\n",
      "    exitcode = _main(fd, parent_sentinel)\n",
      "  File \"/home/ap2934/.conda/envs/torch-cuda/lib/python3.8/multiprocessing/spawn.py\", line 126, in _main\n",
      "    self = reduction.pickle.load(from_parent)\n",
      "AttributeError: Can't get attribute 'objective' on <module '__main__' (built-in)>\n",
      "Traceback (most recent call last):\n",
      "  File \"<string>\", line 1, in <module>\n",
      "  File \"/home/ap2934/.conda/envs/torch-cuda/lib/python3.8/multiprocessing/spawn.py\", line 116, in spawn_main\n",
      "    exitcode = _main(fd, parent_sentinel)\n",
      "  File \"/home/ap2934/.conda/envs/torch-cuda/lib/python3.8/multiprocessing/spawn.py\", line 126, in _main\n",
      "    self = reduction.pickle.load(from_parent)\n",
      "AttributeError: Can't get attribute 'objective' on <module '__main__' (built-in)>\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "ValueError",
     "evalue": "No trials are completed yet.",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mValueError\u001B[0m                                Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[11], line 6\u001B[0m\n\u001B[1;32m      4\u001B[0m study \u001B[38;5;241m=\u001B[39m optuna_distributed\u001B[38;5;241m.\u001B[39mfrom_study(optuna\u001B[38;5;241m.\u001B[39mcreate_study(), client\u001B[38;5;241m=\u001B[39mclient)\n\u001B[1;32m      5\u001B[0m study\u001B[38;5;241m.\u001B[39moptimize(objective, n_trials\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m10\u001B[39m)\n\u001B[0;32m----> 6\u001B[0m \u001B[38;5;28mprint\u001B[39m(\u001B[43mstudy\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mbest_value\u001B[49m)\n",
      "File \u001B[0;32m~/.conda/envs/torch-cuda/lib/python3.8/site-packages/optuna_distributed/study.py:75\u001B[0m, in \u001B[0;36mDistributedStudy.best_value\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m     72\u001B[0m \u001B[38;5;129m@property\u001B[39m\n\u001B[1;32m     73\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mbest_value\u001B[39m(\u001B[38;5;28mself\u001B[39m) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m \u001B[38;5;28mfloat\u001B[39m:\n\u001B[1;32m     74\u001B[0m \u001B[38;5;250m    \u001B[39m\u001B[38;5;124;03m\"\"\"Return the best objective value in the study.\"\"\"\u001B[39;00m\n\u001B[0;32m---> 75\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_study\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mbest_value\u001B[49m\n",
      "File \u001B[0;32m~/.conda/envs/torch-cuda/lib/python3.8/site-packages/optuna/study/study.py:128\u001B[0m, in \u001B[0;36mStudy.best_value\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m    116\u001B[0m \u001B[38;5;129m@property\u001B[39m\n\u001B[1;32m    117\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mbest_value\u001B[39m(\u001B[38;5;28mself\u001B[39m) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m \u001B[38;5;28mfloat\u001B[39m:\n\u001B[1;32m    118\u001B[0m \u001B[38;5;250m    \u001B[39m\u001B[38;5;124;03m\"\"\"Return the best objective value in the study.\u001B[39;00m\n\u001B[1;32m    119\u001B[0m \n\u001B[1;32m    120\u001B[0m \u001B[38;5;124;03m    .. note::\u001B[39;00m\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    125\u001B[0m \n\u001B[1;32m    126\u001B[0m \u001B[38;5;124;03m    \"\"\"\u001B[39;00m\n\u001B[0;32m--> 128\u001B[0m     best_value \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mbest_trial\u001B[49m\u001B[38;5;241m.\u001B[39mvalue\n\u001B[1;32m    129\u001B[0m     \u001B[38;5;28;01massert\u001B[39;00m best_value \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[1;32m    131\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m best_value\n",
      "File \u001B[0;32m~/.conda/envs/torch-cuda/lib/python3.8/site-packages/optuna/study/study.py:157\u001B[0m, in \u001B[0;36mStudy.best_trial\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m    151\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_is_multi_objective():\n\u001B[1;32m    152\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mRuntimeError\u001B[39;00m(\n\u001B[1;32m    153\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mA single best trial cannot be retrieved from a multi-objective study. Consider \u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m    154\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124musing Study.best_trials to retrieve a list containing the best trials.\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m    155\u001B[0m     )\n\u001B[0;32m--> 157\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m copy\u001B[38;5;241m.\u001B[39mdeepcopy(\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_storage\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mget_best_trial\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_study_id\u001B[49m\u001B[43m)\u001B[49m)\n",
      "File \u001B[0;32m~/.conda/envs/torch-cuda/lib/python3.8/site-packages/optuna/storages/_in_memory.py:234\u001B[0m, in \u001B[0;36mInMemoryStorage.get_best_trial\u001B[0;34m(self, study_id)\u001B[0m\n\u001B[1;32m    231\u001B[0m best_trial_id \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_studies[study_id]\u001B[38;5;241m.\u001B[39mbest_trial_id\n\u001B[1;32m    233\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m best_trial_id \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[0;32m--> 234\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mNo trials are completed yet.\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[1;32m    235\u001B[0m \u001B[38;5;28;01melif\u001B[39;00m \u001B[38;5;28mlen\u001B[39m(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_studies[study_id]\u001B[38;5;241m.\u001B[39mdirections) \u001B[38;5;241m>\u001B[39m \u001B[38;5;241m1\u001B[39m:\n\u001B[1;32m    236\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mRuntimeError\u001B[39;00m(\n\u001B[1;32m    237\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mBest trial can be obtained only for single-objective optimization.\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m    238\u001B[0m     )\n",
      "\u001B[0;31mValueError\u001B[0m: No trials are completed yet."
     ]
    }
   ],
   "source": [
    "import optuna_distributed\n",
    "from dask.distributed import Client\n",
    "client = None\n",
    "study = optuna_distributed.from_study(optuna.create_study(), client=client)\n",
    "study.optimize(objective, n_trials=10) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b67538d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03302877",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch-cuda",
   "language": "python",
   "name": "torch-cuda"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
